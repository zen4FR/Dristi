# DRISTI â€” AI-Based Assistive Vision System

**Semester project â€” Kathmandu University**

DRISTI helps visually impaired users understand their surroundings by converting an input **photo** into a spoken description using computer vision and lightweight transformer models.

---

## ðŸ”Ž Project Summary

- **Input:** single image (photo)  
- **Pipeline:** object detection (YOLOv8) â†’ depth estimation (MiDaS) â†’ image captioning (BLIP) â†’ fusion â†’ text-to-speech  
- **Output:** short spoken description + optional annotated image and JSON output  
- **Semester scope:** Photo-only MVP. Video/real-time will be implemented in a future semester.

---

## ðŸš€ Features

- Fast object detection with **YOLOv8**  
- Relative depth estimation using **MiDaS / DepthAnything**  
- Scene captioning via **BLIP** (Vision + Transformer)  
- Lightweight fusion engine (template-based NLG) â€” no large LLM required  
- Text-to-speech via **Coqui TTS** or system TTS  
- Modular code structure for easy extension

---




## âš™ï¸ Quick Setup

> Tested on Python 3.9+. Use a virtual environment.

```bash
# 1. Clone repository
git clone https://github.com/<your-username>/dristi.git
cd dristi

# 2. Create & activate venv
python -m venv venv
# Windows
venv\Scripts\activate
# Linux / macOS
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

Requirements (example)
ultralytics
torch
torchvision
opencv-python
transformers
timm
numpy
coqui-tts



ðŸ§  Design / Architecture

See docs/architecture.md for the full architecture description, data flow diagram, and deployment plan.
Short flow:
Image â†’ Preprocessing â†’ YOLOv8 â†’ MiDaS â†’ BLIP â†’ Fusion â†’ TTS â†’ Audio